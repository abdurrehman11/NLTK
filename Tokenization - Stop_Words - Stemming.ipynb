{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Corpus**  - Body of text, singular. **Corpora** is the plural of this. Example: A collection of medical journals.\n",
    "\n",
    "**Lexicon** - Words and their meanings. Example: English dictionary. Consider, however, that various fields will have different lexicons. For example: To a financial investor, the first meaning for the word **Bull** is someone who is confident about the market, as compared to the common English lexicon, where the first meaning for the word **Bull** is an animal. As such, there is a special lexicon for financial investors, doctors, children, mechanics, and so on.\n",
    "\n",
    "**Token** - Each **entity** that is a part of whatever was split up based on rules. For examples, each word is a token when a sentence is \"tokenized\" into words. Each sentence can also be a token, if you tokenized the sentences out of a paragraph.\n",
    "\n",
    "**Tokenization** - There are two types of tokenization: 1- Word tokenization .... 2- Sentence tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_paragraph = (\"Hello, Mr. NLTK, I'm expecting a lot from you. This is my first task in nltk. Hoping to get a lot of \"\n",
    "                     \"good stuff from nltk.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Hello, Mr. NLTK, I'm expecting a lot from you.\",\n",
       " 'This is my first task in nltk.',\n",
       " 'Hoping to get a lot of good stuff from nltk.']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokenize paragraph into sentence tokens\n",
    "sent_tokenize(example_paragraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " ',',\n",
       " 'Mr.',\n",
       " 'NLTK',\n",
       " ',',\n",
       " 'I',\n",
       " \"'m\",\n",
       " 'expecting',\n",
       " 'a',\n",
       " 'lot',\n",
       " 'from',\n",
       " 'you',\n",
       " '.',\n",
       " 'This',\n",
       " 'is',\n",
       " 'my',\n",
       " 'first',\n",
       " 'task',\n",
       " 'in',\n",
       " 'nltk',\n",
       " '.',\n",
       " 'Hoping',\n",
       " 'to',\n",
       " 'get',\n",
       " 'a',\n",
       " 'lot',\n",
       " 'of',\n",
       " 'good',\n",
       " 'stuff',\n",
       " 'from',\n",
       " 'nltk',\n",
       " '.']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokenize paragraph into word tokens (nltk treat punctuations as independent word/token)\n",
    "word_tokenize(example_paragraph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: tokenization can be done using regular expressions as well but as the structure of text becomes more complex, it\n",
    "becomes too difficult to write regex for that so here nltk comes with lot of easiness plus efficent as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Stop Words** - Stop the most common/occuring words from a sentence. You can add the words to stop words which you want to stop. Let;s see an example,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This', 'example', 'showing', 'stop', 'word', 'filteration', '.']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_sentence = \"This is an example showing off stop word filteration.\"\n",
    "\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "words = word_tokenize(example_sentence)\n",
    "\n",
    "filtered = [w for w in words if w not in stop_words]\n",
    "filtered"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Stemming** - The idea of stemming is a sort of normalizing method. Many variations of words carry the same meaning, other than when tense is involved. The reason why we stem is to shorten the lookup, and normalize sentences.\n",
    "\n",
    "Consider an example,\n",
    "\n",
    "1- I was taking a ride in the car.\n",
    "2- I was riding in the car.\n",
    "\n",
    "In above two sentences, both ride and riding have same meaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I',\n",
       " 'wa',\n",
       " 'take',\n",
       " 'a',\n",
       " 'ride',\n",
       " 'in',\n",
       " 'the',\n",
       " 'car',\n",
       " '.',\n",
       " 'I',\n",
       " 'wa',\n",
       " 'ride',\n",
       " 'in',\n",
       " 'the',\n",
       " 'car',\n",
       " '.']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ps = PorterStemmer()\n",
    "\n",
    "example_sentence = \"I was taking a ride in the car. I was riding in the car.\"\n",
    "words = word_tokenize(example_sentence)\n",
    "\n",
    "stemmed = [ps.stem(w) for w in words]\n",
    "stemmed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
